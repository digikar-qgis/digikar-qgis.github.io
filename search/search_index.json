{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Downloads Subscribe to updates Registration Brought to you by the LASTIG team of the IGN (the French National Mapping Agency), the R&D Lab . of the EPITA (French engineering school in computer science), and the Center of Historical Studies of the EHESS (French graduate schools of social sciences). Important dates From To / On Title Tasks 2020-11-18 Train and validation sets available all 2020-11-18 2021-04-05 Training phase all 2021-03-31 Registration deadline for competition participants all 2021-04-05 Test datasets available 1 2021-04-05 2021-04-09 Test phase 1 2021-04-09 Submission deadline for results 1 2021-04-12 Test datasets available 2&3 2021-04-12 2021-04-16 Test phase 2&3 2021-04-16 Submission deadline for results 2&3 2021-04-12 2021-04-23 Write short method description all 2021-04-23 Method descriptions due all 2021-07-01 Full data disclosure: round truth for test set, participant results all About the competition This competition consists in solving several challenges which arise during the digitization of historical maps. We are particularly interested in a large map series consisting in many Paris atlases over half a century (1860's-1940's). For each year, a set of approximately 20 sheets forms a tiled view of the city. Such maps are highly detailed and very accurate even in modern standards. The graphical nature of the content is visible in the full map sheet illustrated below. A sample map sheet ( link to larger version ) This material provides a very valuable resource for historians and a rich body of scientific challenges for the document analysis and recognition (DAR) community: map-related challenges (overlapping of many information layers) and document-related ones (document preservation). Excerpt showing map-related challenges Excerpt showing document-related challenges We expect this competition to provide solutions for three levels of scientific problems: Task 1: Detection of building blocks Task 2: Segmentation of map content within map sheets Task 3: Localization of graticule lines intersections These tasks are necessary step during the digitization of historical maps, ie when gradually turning a raster image into a set of vector geometries projected onto a particular geographical coordinate reference system. Automatic approaches with good generalization power will provide an enormous gain for the Geographical Information Systems (GIS) communities looking for solutions when digitizing old maps, and create a great potential for many historical studies. Key strengths for participants All datasets and evaluation tools are released with an open licence as soon as they are available. Winners of each task will be invited to co-author the report paper. We propose unsolved research problems with an important potential impact .","title":"Overview"},{"location":"#overview","text":"Downloads Subscribe to updates Registration Brought to you by the LASTIG team of the IGN (the French National Mapping Agency), the R&D Lab . of the EPITA (French engineering school in computer science), and the Center of Historical Studies of the EHESS (French graduate schools of social sciences).","title":"Overview"},{"location":"#important-dates","text":"From To / On Title Tasks 2020-11-18 Train and validation sets available all 2020-11-18 2021-04-05 Training phase all 2021-03-31 Registration deadline for competition participants all 2021-04-05 Test datasets available 1 2021-04-05 2021-04-09 Test phase 1 2021-04-09 Submission deadline for results 1 2021-04-12 Test datasets available 2&3 2021-04-12 2021-04-16 Test phase 2&3 2021-04-16 Submission deadline for results 2&3 2021-04-12 2021-04-23 Write short method description all 2021-04-23 Method descriptions due all 2021-07-01 Full data disclosure: round truth for test set, participant results all","title":"Important dates"},{"location":"#about-the-competition","text":"This competition consists in solving several challenges which arise during the digitization of historical maps. We are particularly interested in a large map series consisting in many Paris atlases over half a century (1860's-1940's). For each year, a set of approximately 20 sheets forms a tiled view of the city. Such maps are highly detailed and very accurate even in modern standards. The graphical nature of the content is visible in the full map sheet illustrated below. A sample map sheet ( link to larger version ) This material provides a very valuable resource for historians and a rich body of scientific challenges for the document analysis and recognition (DAR) community: map-related challenges (overlapping of many information layers) and document-related ones (document preservation). Excerpt showing map-related challenges Excerpt showing document-related challenges We expect this competition to provide solutions for three levels of scientific problems: Task 1: Detection of building blocks Task 2: Segmentation of map content within map sheets Task 3: Localization of graticule lines intersections These tasks are necessary step during the digitization of historical maps, ie when gradually turning a raster image into a set of vector geometries projected onto a particular geographical coordinate reference system. Automatic approaches with good generalization power will provide an enormous gain for the Geographical Information Systems (GIS) communities looking for solutions when digitizing old maps, and create a great potential for many historical studies.","title":"About the competition"},{"location":"#key-strengths-for-participants","text":"All datasets and evaluation tools are released with an open licence as soon as they are available. Winners of each task will be invited to co-author the report paper. We propose unsolved research problems with an important potential impact .","title":"Key strengths for participants"},{"location":"contact/","text":"Contact Single Contact Address You can contact the organizers by sending an email to: icdar21-mapseg-contact(at)googlegroups.com You can also create an issue on our tracker . Subscribe to Updates You can subscribe to the announcement list ( you can unsubscribe at any time ) by sending an email to: icdar21-mapseg-announcements+subscribe@googlegroups.com or by going to: https://groups.google.com/g/icdar21-mapseg-announcements Organizers Edwin Carlinet Associate professor of computer vision at the EPITA Computer Science school. Joseph Chazalon Associate professor of computer vision at the EPITA Computer Science school. Yizi Chen PhD candidate at IGN/ENSG/Univ. Gustave Eiffel and LRDE/EPITA. Bertrand Dum\u00e9nieu Research engineer in geographical information sciences and geospatial humanities at EHESS. Thierry G\u00e9raud Full professor of computer vision at the EPITA Computer Science school. LRDE Team leader. Cl\u00e9ment Mallet Senior researcher in geographical information sciences at IGN/ENSG/Univ. Gustave Eiffel. Julien Perret Senior researcher in geographical information sciences at IGN/ENSG/Univ. Gustave Eiffel and associated researcher at EHESS. Organizations LRDE / EPITA The EPITA is a renowned engineering school in computer science. Its R&D Lab. (LRDE) is specialized is efficient image processing, with a focus on document and medical image analysis. LASTIG / IGN IGN is the French National Mapping Agency, owning more than 3 centuries of map archives and one century of aerial images over France and foreign countries. The research from LASTIG (one of IGN research labs) covers the complete lifecycle of spatial data from the capture to the visualization, including modelling, integration and analysis of spatial data with a special interest on spatio-temporal reference data. CRH / EHESS EHESS is one of the most selective and prestigious graduate schools of social sciences in Paris, France. Its researchers are affiliated to 35 Research Centers covering an extensive range of disciplines: history, sociology, anthropology, economics, geography, archaeology, psychology, and linguistics.","title":"Contact"},{"location":"contact/#contact","text":"","title":"Contact"},{"location":"contact/#single-contact-address","text":"You can contact the organizers by sending an email to: icdar21-mapseg-contact(at)googlegroups.com You can also create an issue on our tracker .","title":"Single Contact Address"},{"location":"contact/#subscribe-to-updates","text":"You can subscribe to the announcement list ( you can unsubscribe at any time ) by sending an email to: icdar21-mapseg-announcements+subscribe@googlegroups.com or by going to: https://groups.google.com/g/icdar21-mapseg-announcements","title":"Subscribe to Updates"},{"location":"contact/#organizers","text":"Edwin Carlinet Associate professor of computer vision at the EPITA Computer Science school. Joseph Chazalon Associate professor of computer vision at the EPITA Computer Science school. Yizi Chen PhD candidate at IGN/ENSG/Univ. Gustave Eiffel and LRDE/EPITA. Bertrand Dum\u00e9nieu Research engineer in geographical information sciences and geospatial humanities at EHESS. Thierry G\u00e9raud Full professor of computer vision at the EPITA Computer Science school. LRDE Team leader. Cl\u00e9ment Mallet Senior researcher in geographical information sciences at IGN/ENSG/Univ. Gustave Eiffel. Julien Perret Senior researcher in geographical information sciences at IGN/ENSG/Univ. Gustave Eiffel and associated researcher at EHESS.","title":"Organizers"},{"location":"contact/#organizations","text":"LRDE / EPITA The EPITA is a renowned engineering school in computer science. Its R&D Lab. (LRDE) is specialized is efficient image processing, with a focus on document and medical image analysis. LASTIG / IGN IGN is the French National Mapping Agency, owning more than 3 centuries of map archives and one century of aerial images over France and foreign countries. The research from LASTIG (one of IGN research labs) covers the complete lifecycle of spatial data from the capture to the visualization, including modelling, integration and analysis of spatial data with a special interest on spatio-temporal reference data. CRH / EHESS EHESS is one of the most selective and prestigious graduate schools of social sciences in Paris, France. Its researchers are affiliated to 35 Research Centers covering an extensive range of disciplines: history, sociology, anthropology, economics, geography, archaeology, psychology, and linguistics.","title":"Organizations"},{"location":"downloads/","text":"Downloads Dataset (train and validation sets) The train and validation sets for tasks 1, 2 and 3 is available as an archive. You can currently download them here: Mirror 1 MD5 checksums: c54c35d132701bc5e6ddd5c70fc854b3 icdar21-mapseg-v1.0.0-trainval-20201114a.zip Dataset (test sets) The test sets (inputs only) will be released at the beginning of the test phase for each task: task 1: 2021-04-05 tasks 2 and 3: 2021-04-12 Please subscribe to updates to be notified when they are available. Evaluation tools Evaluation tools are currently being polished and will be released shortly (open source Python scripts). Participants will be able to evaluate the performance of their systems by themselves. Please subscribe to updates to be notified when they are available. Competition results Competition results computed by participants will be made available publicly on this website, so that anyone can recompute the results and derive the final ranking. Please subscribe to updates to be notified when they are available.","title":"Downloads"},{"location":"downloads/#downloads","text":"","title":"Downloads"},{"location":"downloads/#dataset-train-and-validation-sets","text":"The train and validation sets for tasks 1, 2 and 3 is available as an archive. You can currently download them here: Mirror 1 MD5 checksums: c54c35d132701bc5e6ddd5c70fc854b3 icdar21-mapseg-v1.0.0-trainval-20201114a.zip","title":"Dataset (train and validation sets)"},{"location":"downloads/#dataset-test-sets","text":"The test sets (inputs only) will be released at the beginning of the test phase for each task: task 1: 2021-04-05 tasks 2 and 3: 2021-04-12 Please subscribe to updates to be notified when they are available.","title":"Dataset (test sets)"},{"location":"downloads/#evaluation-tools","text":"Evaluation tools are currently being polished and will be released shortly (open source Python scripts). Participants will be able to evaluate the performance of their systems by themselves. Please subscribe to updates to be notified when they are available.","title":"Evaluation tools"},{"location":"downloads/#competition-results","text":"Competition results computed by participants will be made available publicly on this website, so that anyone can recompute the results and derive the final ranking. Please subscribe to updates to be notified when they are available.","title":"Competition results"},{"location":"registration/","text":"Registration Loading\u2026","title":"Registration"},{"location":"registration/#registration","text":"Loading\u2026","title":"Registration"},{"location":"rules/","text":"Competition Rules Dear participants, in order to create valuable results and provide a fair competition experience for all participants, please respect those rules: You can participate to 1, 2 or 3 tasks: they will be awarded separately. You must register and process the test set in time for your method to be evaluated. You must submit your method short description in time for your method to be included in the result paper. Your method must not include any manual annotation process. You must train on the training set or publicly available data exclusively. You must clearly indicate in your method presentation the training and validation sets you used. You should calibrate all your parameters using the validation set. Not doing so will be pointed out in the report. You may use the complete dataset for training. In particular, you can use the training sets for tasks 2 and 3 as unsupervised training examples for task 1. If you do so, please report it in your method's description. You must report in your method's description any redundancy in data that you make use of. We will not ask you to send code or binaries, but we encourage you to release your method under an open-source license after the competition.","title":"Competition Rules"},{"location":"rules/#competition-rules","text":"Dear participants, in order to create valuable results and provide a fair competition experience for all participants, please respect those rules: You can participate to 1, 2 or 3 tasks: they will be awarded separately. You must register and process the test set in time for your method to be evaluated. You must submit your method short description in time for your method to be included in the result paper. Your method must not include any manual annotation process. You must train on the training set or publicly available data exclusively. You must clearly indicate in your method presentation the training and validation sets you used. You should calibrate all your parameters using the validation set. Not doing so will be pointed out in the report. You may use the complete dataset for training. In particular, you can use the training sets for tasks 2 and 3 as unsupervised training examples for task 1. If you do so, please report it in your method's description. You must report in your method's description any redundancy in data that you make use of. We will not ask you to send code or binaries, but we encourage you to release your method under an open-source license after the competition.","title":"Competition Rules"},{"location":"tasks/task1/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } Task 1: Detect Building Blocks This task consist in detecting a set of closed shapes (building blocks) in the map image. Building blocks are coarser map objects which can regroup several elements. Detecting these objects is a critical step in the digitization of historical maps because it provides essential components of a city. Each building block is symbolized by a closed which can enclose other objects and lines. Building blocks are surrounded by streets, rivers fortification wall or others, and are never directly connected. Building blocks can sometimes be reduced to a single spacial building, symbolized by diagonal hatched areas. Given the image of a complete map sheet and a mask of the map area, you need to detect each building block, as illustrated below on a excerpt of the input. Excerpt of the input for task 1 Excerpt of the expected output for task 1 We identified the following challenges: Building blocks can have very variable sizes. They can be reduced to a single building (diagonal hatched area). Their contour can be damaged (ie non-closed). Several other layers of information can be overlaid on building block outlines (text, railways, underground lines, graticule lines among others). There may be decompositions inside a building block, increasing the number of edges to filter. Building blocks may be large empty areas only surrounded by a contour. The lack of texture information and the variable sizes may be challenging to multiscale texture methods like convolutional neural networks. Producing closed contours requires to embed strong guarantees in the method. Input The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images cropped to the relevant area for which the non-relevant area is replaced by black pixels, as illustrated below. Those images can be large (8000x8000 pixels). Sample input for task 1: non relevant pixels are replaced by black pixels. To help participants identify the non-relevant pixels, we also provide a mask image for which non-relevant pixels have value 0 and relevant pixels have value 255 , as illustrated below. Theses masks are the expected output for task 2 (cropped to the relevant part). Extra input mask for task 1 (exact same format as the output for task 2): indicates the masked content. Ground truth and Expected outputs Expected output for this task is a binary mask of the building blocks. It must be stored in PNG (lossless) format with an 8-bit single channel. Background must be indicated with pixel value 0, and building block areas with pixel value 255. We will threshold the image to discard any other value. The resulting image should look like the one below, which is the expected output for the sample input previously shown. Sample output for task 1 Results need to be output in a PNG file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.png . Dataset Content for task 1 is located in the folder named 1-detbblocks in the dataset archive. File naming conventions Train, validation and test folder (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. This image was cropped to map content and irrelevant content (masked out by the mask file described below) are set to black (RGB= (0,0,0) ). Those images can be large (10000x10000 pixels). example: 1-detbblocks/train/101-INPUT.jpg ${SUBSET}/${NNN}-INPUT-MASK.png : PNG GREY image containing a mask of the map area (same size as input). While the image was cropped to the meaningful area, some elements need to be discarded (map legend for instance). Map area is indicated by pixels of value 255 ; only predictions within this area is to be kept. example: 1-detbblocks/train/101-INPUT-MASK.png ${SUBSET}/${NNN}-OUTPUT-GT.png : PNG GREY image containing a mask of expected building blocks (same size as input). Building blocks are indicated by pixels of value 255 ; all other pixels (background and irrelevant) are set to 0 . example: 1-detbblocks/train/101-OUTPUT-GT.png Number of elements per set train: 1 image validation: 1 image test: 3 images Evaluation Evaluation tools and illustrative notebooks provide participants with more details than the summary below. Please subscribe to updates to be notified when they are available. For each map sheet, we will extract the connected components from the predicted label mask. Based on this mask, we will compute the intersection over union (IoU) between each ground truth component and each predicted one, and retain only the matches with a value of at least 0.5 0.5 0 . 5 . When the IoU is strictly superior to 0.5 0.5 0 . 5 we have 1-to-1 matches between ground truth components and predicted ones, and this enables the computation of prediction, recall and F 1 F_1 F 1 \u200b scores. We will compute the F 1 F_1 F 1 \u200b values for each possible IoU threshold in ] 0.5 , 1 ] ]0.5, 1] ] 0 . 5 , 1 ] and compute the area under the resulting curve. Such score will not only be free of any threshold, it will also be insensitive to shape area; thus weighting large and small shapes equally and provided an accurate measure of the number of objects properly detected. Finally, we will compute the average of the measures for all individual map images to produce a global indicator. The resulting measure is a float value between 0 and 0.5 0.5 0 . 5 . A high value is better.","title":"Task 1: Detect Building Blocks"},{"location":"tasks/task1/#task-1-detect-building-blocks","text":"This task consist in detecting a set of closed shapes (building blocks) in the map image. Building blocks are coarser map objects which can regroup several elements. Detecting these objects is a critical step in the digitization of historical maps because it provides essential components of a city. Each building block is symbolized by a closed which can enclose other objects and lines. Building blocks are surrounded by streets, rivers fortification wall or others, and are never directly connected. Building blocks can sometimes be reduced to a single spacial building, symbolized by diagonal hatched areas. Given the image of a complete map sheet and a mask of the map area, you need to detect each building block, as illustrated below on a excerpt of the input. Excerpt of the input for task 1 Excerpt of the expected output for task 1 We identified the following challenges: Building blocks can have very variable sizes. They can be reduced to a single building (diagonal hatched area). Their contour can be damaged (ie non-closed). Several other layers of information can be overlaid on building block outlines (text, railways, underground lines, graticule lines among others). There may be decompositions inside a building block, increasing the number of edges to filter. Building blocks may be large empty areas only surrounded by a contour. The lack of texture information and the variable sizes may be challenging to multiscale texture methods like convolutional neural networks. Producing closed contours requires to embed strong guarantees in the method.","title":"Task 1: Detect Building Blocks"},{"location":"tasks/task1/#input","text":"The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images cropped to the relevant area for which the non-relevant area is replaced by black pixels, as illustrated below. Those images can be large (8000x8000 pixels). Sample input for task 1: non relevant pixels are replaced by black pixels. To help participants identify the non-relevant pixels, we also provide a mask image for which non-relevant pixels have value 0 and relevant pixels have value 255 , as illustrated below. Theses masks are the expected output for task 2 (cropped to the relevant part). Extra input mask for task 1 (exact same format as the output for task 2): indicates the masked content.","title":"Input"},{"location":"tasks/task1/#ground-truth-and-expected-outputs","text":"Expected output for this task is a binary mask of the building blocks. It must be stored in PNG (lossless) format with an 8-bit single channel. Background must be indicated with pixel value 0, and building block areas with pixel value 255. We will threshold the image to discard any other value. The resulting image should look like the one below, which is the expected output for the sample input previously shown. Sample output for task 1 Results need to be output in a PNG file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.png .","title":"Ground truth and Expected outputs"},{"location":"tasks/task1/#dataset","text":"Content for task 1 is located in the folder named 1-detbblocks in the dataset archive.","title":"Dataset"},{"location":"tasks/task1/#file-naming-conventions","text":"Train, validation and test folder (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. This image was cropped to map content and irrelevant content (masked out by the mask file described below) are set to black (RGB= (0,0,0) ). Those images can be large (10000x10000 pixels). example: 1-detbblocks/train/101-INPUT.jpg ${SUBSET}/${NNN}-INPUT-MASK.png : PNG GREY image containing a mask of the map area (same size as input). While the image was cropped to the meaningful area, some elements need to be discarded (map legend for instance). Map area is indicated by pixels of value 255 ; only predictions within this area is to be kept. example: 1-detbblocks/train/101-INPUT-MASK.png ${SUBSET}/${NNN}-OUTPUT-GT.png : PNG GREY image containing a mask of expected building blocks (same size as input). Building blocks are indicated by pixels of value 255 ; all other pixels (background and irrelevant) are set to 0 . example: 1-detbblocks/train/101-OUTPUT-GT.png","title":"File naming conventions"},{"location":"tasks/task1/#number-of-elements-per-set","text":"train: 1 image validation: 1 image test: 3 images","title":"Number of elements per set"},{"location":"tasks/task1/#evaluation","text":"Evaluation tools and illustrative notebooks provide participants with more details than the summary below. Please subscribe to updates to be notified when they are available. For each map sheet, we will extract the connected components from the predicted label mask. Based on this mask, we will compute the intersection over union (IoU) between each ground truth component and each predicted one, and retain only the matches with a value of at least 0.5 0.5 0 . 5 . When the IoU is strictly superior to 0.5 0.5 0 . 5 we have 1-to-1 matches between ground truth components and predicted ones, and this enables the computation of prediction, recall and F 1 F_1 F 1 \u200b scores. We will compute the F 1 F_1 F 1 \u200b values for each possible IoU threshold in ] 0.5 , 1 ] ]0.5, 1] ] 0 . 5 , 1 ] and compute the area under the resulting curve. Such score will not only be free of any threshold, it will also be insensitive to shape area; thus weighting large and small shapes equally and provided an accurate measure of the number of objects properly detected. Finally, we will compute the average of the measures for all individual map images to produce a global indicator. The resulting measure is a float value between 0 and 0.5 0.5 0 . 5 . A high value is better.","title":"Evaluation"},{"location":"tasks/task2/","text":"Task 2: Segment Map Content Area This task consist in segmenting the map content from the rest of the sheet. This is a rather classical document analysis task as it consist in focusing on the relevant area in order to perform a dedicated analysis. In our case, task 1 would be the following stage in the pipeline. Given the image of a complete map sheet, you need to locate the boundary of the map content, as illustrated below. Illustration of expected outputs for task 2: green area is the map content and red hatched area is the background. We identified the following challenges: The map area usually is well separated from the other elements (title, legend, scale\u2026) by several frames but sometimes map contents exceeds the frame for some large objects. The frame itself is not straight and can be damaged. Input Note that the inputs for this task are the same as task 3. The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images. Those images can be large (10000x10000 pixels). Sample input for task 3 Ground truth and Expected outputs Expected output for this task is a binary mask of the map content area. It must be stored in PNG (lossless) format with an 8-bit single channel. Background must be indicated with pixel value 0, and map content area with pixel value 255. We will threshold the image to discard any other value. The resulting image should look like the one below, which is the expected output for the sample input previously shown. Sample output for task 2 Results need to be output in a PNG file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.png . Dataset Content for task 2 is located in the folder named 2-segmaparea in the dataset archive. File naming conventions Train, validation and test folder (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. There are complete map sheet images. Those images can be large (10000x10000 pixels). example: 2-segmaparea/train/101-INPUT.jpg ${SUBSET}/${NNN}-OUTPUT-GT.png : PNG GREY image containing a mask of expected map are (same size as input). Map area is indicated by pixels of value 255 ; all other pixels are set to 0 . example: 2-segmaparea/train/101-OUTPUT-GT.png Number of elements per set train: 26 images validation: 6 images test: 97 images Evaluation Evaluation tools and illustrative notebooks provide participants with more details than the summary below. Please subscribe to updates to be notified when they are available. To evaluate the quality of the segmentation, we will compute the Hausdorff distance between the expected shape do detect and the predicted one. This implies that there is only one (connected, without hole) shape to detect. This measure has the advantage over the \"intersection over union\", \"Jaccard index\" and other area measures that it keeps a good \"contrast\" between results in the case of large objects (because there is no normalization by the area). More specifically, we will use the \"Hausdorff 95\" variant which discards the 5 percentiles of higher values (assumed to be outliers) to produce a more stable measure. Finally, we will compute the average of the measures for all individual map images to produce a global indicator with a confidence measure. The resulting measure is a float value between 0 and a large value. A lower value is better.","title":"Task 2: Segment Map Content Area"},{"location":"tasks/task2/#task-2-segment-map-content-area","text":"This task consist in segmenting the map content from the rest of the sheet. This is a rather classical document analysis task as it consist in focusing on the relevant area in order to perform a dedicated analysis. In our case, task 1 would be the following stage in the pipeline. Given the image of a complete map sheet, you need to locate the boundary of the map content, as illustrated below. Illustration of expected outputs for task 2: green area is the map content and red hatched area is the background. We identified the following challenges: The map area usually is well separated from the other elements (title, legend, scale\u2026) by several frames but sometimes map contents exceeds the frame for some large objects. The frame itself is not straight and can be damaged.","title":"Task 2: Segment Map Content Area"},{"location":"tasks/task2/#input","text":"Note that the inputs for this task are the same as task 3. The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images. Those images can be large (10000x10000 pixels). Sample input for task 3","title":"Input"},{"location":"tasks/task2/#ground-truth-and-expected-outputs","text":"Expected output for this task is a binary mask of the map content area. It must be stored in PNG (lossless) format with an 8-bit single channel. Background must be indicated with pixel value 0, and map content area with pixel value 255. We will threshold the image to discard any other value. The resulting image should look like the one below, which is the expected output for the sample input previously shown. Sample output for task 2 Results need to be output in a PNG file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.png .","title":"Ground truth and Expected outputs"},{"location":"tasks/task2/#dataset","text":"Content for task 2 is located in the folder named 2-segmaparea in the dataset archive.","title":"Dataset"},{"location":"tasks/task2/#file-naming-conventions","text":"Train, validation and test folder (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. There are complete map sheet images. Those images can be large (10000x10000 pixels). example: 2-segmaparea/train/101-INPUT.jpg ${SUBSET}/${NNN}-OUTPUT-GT.png : PNG GREY image containing a mask of expected map are (same size as input). Map area is indicated by pixels of value 255 ; all other pixels are set to 0 . example: 2-segmaparea/train/101-OUTPUT-GT.png","title":"File naming conventions"},{"location":"tasks/task2/#number-of-elements-per-set","text":"train: 26 images validation: 6 images test: 97 images","title":"Number of elements per set"},{"location":"tasks/task2/#evaluation","text":"Evaluation tools and illustrative notebooks provide participants with more details than the summary below. Please subscribe to updates to be notified when they are available. To evaluate the quality of the segmentation, we will compute the Hausdorff distance between the expected shape do detect and the predicted one. This implies that there is only one (connected, without hole) shape to detect. This measure has the advantage over the \"intersection over union\", \"Jaccard index\" and other area measures that it keeps a good \"contrast\" between results in the case of large objects (because there is no normalization by the area). More specifically, we will use the \"Hausdorff 95\" variant which discards the 5 percentiles of higher values (assumed to be outliers) to produce a more stable measure. Finally, we will compute the average of the measures for all individual map images to produce a global indicator with a confidence measure. The resulting measure is a float value between 0 and a large value. A lower value is better.","title":"Evaluation"},{"location":"tasks/task3/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } Task 3: Locate Graticule Lines Intersections This task consist in locating the intersection points of graticule lines. Graticule lines are lines indicating the North/South/East/West major coordinates in the map. They are drawn every 1000 meters in each direction and overlap with the rest of the map content. Their intersections are very useful to geo-reference the map image, ie for projecting map content in a modern geographical coordinate reference system . Given the image of a complete map sheet, you need to locate the intersection points of such lines, as illustrated below. Illustration of expected outputs for task 3: dashed green lines are the graticule lines and red dots are the intersections points to locate. We identified the following challenges: Theses lines, while crossing the map from edge to edge, may not be horizontal and vertical but sometimes diagonal. They can be damaged and locally curbed. They overlap numerous map content and can touch other parallel segments. Input Note that the inputs for this task are the same as task 2. The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images. Those images can be large (10000x10000 pixels). Sample input for task 3 Close view of a sample input for task 3 Ground truth and Expected outputs Expected output for this task is a list of coordinates (in image referential, ie. 0,0 at top left, x axis pointing to the right and y axis pointing downward). Coordinates need to be output in a CSV file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.csv . The format of the CSV is the same as the one of the ground truth described below. The CSV should look like: x,y 2379.0,2338.0 2373.2,4708.2 4744.2,2332.8 4736.5,4724.5 Each CSV line should indicate an intersection like the one illustrated below. Close view of an intersection point to detection for task 3: you can see that many graphical elements can disrupt the detection of such element. Dataset Content for task 3 is located in the folder named 3-locglinesinter in the dataset archive. WARNING: because the inputs of this task are exactly the same as task 2, we did not duplicate them. Please copy or link the 2-segmaparea/{train,validation,test}/*-INPUT.jpg files accordingly. File naming conventions Train, validation and test folders (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. There are complete map sheet images. Those images can be large (10000x10000 pixels). example: 2-segmaparea/train/101-INPUT.jpg ${SUBSET}/${NNN}-OUTPUT-GT.csv : CSV file containing a list of intersection coordinates (detailed below). example: 2-segmaparea/train/101-OUTPUT-GT.csv Number of elements per set train: 26 images validation: 6 images test: 97 images CSV file format first line: header (always x,y ) other lines: two floats delimiter: comma ( , ) float format: dot ( . ) as decimal separator, 1 digit after the dot (sub-pixel accuracy) coordinate system: image: (0,0) is at top left, x axis points to the right and y axis points downward CSV example: x,y 2379.0,2338.0 2373.2,4708.2 4744.2,2332.8 4736.5,4724.5 Evaluation Evaluation tools and illustrative notebooks provide participants with more details than the summary below. Please subscribe to updates to be notified when they are available. For each map sheet (ie for each CSV result), we will compare the predicted coordinates with the expected ones. We will compute for each map sheet the number of correct predictions for each possible distance threshold: a predicted point will be considered as a correct detection if it is the closest predicted point of a ground truth (expected) point and the distance between the expected point and the predicted one is smaller than a given threshold we will consider all possible thresholds between 0 and 50 pixels, which roughly represents 20 meters on the maps and gives an upper limit over which the registration would be seriously disrupted. For each possible threshold we can compute the number of correct predictions, the number of incorrect ones (the complement of the prediction set) and the number of expected elements. This allows us to plot a F 0.5 F_{0.5} F 0 . 5 \u200b score vs threshold curve for a range of thresholds. The F \u03b2 F_{\\beta} F \u03b2 \u200b score with \u03b2 = 0.5 \\beta=0.5 \u03b2 = 0 . 5 weights recall lower than precision because for this task it takes several good detections to correct a wrong one in the final registration. We will take the area under this \" F 0.5 F_{0.5} F 0 . 5 \u200b score vs threshold \" curve as a performance indicator: such indicator blends two indicators: point detection and spatial accuracy. Finally, we will compute the average of the measures for all individual map images to produce a global indicator with a confidence measure. The resulting measure is a float value between 0 and 1. A higher value is better.","title":"Task 3: Locate Graticule Lines Intersections"},{"location":"tasks/task3/#task-3-locate-graticule-lines-intersections","text":"This task consist in locating the intersection points of graticule lines. Graticule lines are lines indicating the North/South/East/West major coordinates in the map. They are drawn every 1000 meters in each direction and overlap with the rest of the map content. Their intersections are very useful to geo-reference the map image, ie for projecting map content in a modern geographical coordinate reference system . Given the image of a complete map sheet, you need to locate the intersection points of such lines, as illustrated below. Illustration of expected outputs for task 3: dashed green lines are the graticule lines and red dots are the intersections points to locate. We identified the following challenges: Theses lines, while crossing the map from edge to edge, may not be horizontal and vertical but sometimes diagonal. They can be damaged and locally curbed. They overlap numerous map content and can touch other parallel segments.","title":"Task 3: Locate Graticule Lines Intersections"},{"location":"tasks/task3/#input","text":"Note that the inputs for this task are the same as task 2. The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images. Those images can be large (10000x10000 pixels). Sample input for task 3 Close view of a sample input for task 3","title":"Input"},{"location":"tasks/task3/#ground-truth-and-expected-outputs","text":"Expected output for this task is a list of coordinates (in image referential, ie. 0,0 at top left, x axis pointing to the right and y axis pointing downward). Coordinates need to be output in a CSV file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.csv . The format of the CSV is the same as the one of the ground truth described below. The CSV should look like: x,y 2379.0,2338.0 2373.2,4708.2 4744.2,2332.8 4736.5,4724.5 Each CSV line should indicate an intersection like the one illustrated below. Close view of an intersection point to detection for task 3: you can see that many graphical elements can disrupt the detection of such element.","title":"Ground truth and Expected outputs"},{"location":"tasks/task3/#dataset","text":"Content for task 3 is located in the folder named 3-locglinesinter in the dataset archive. WARNING: because the inputs of this task are exactly the same as task 2, we did not duplicate them. Please copy or link the 2-segmaparea/{train,validation,test}/*-INPUT.jpg files accordingly.","title":"Dataset"},{"location":"tasks/task3/#file-naming-conventions","text":"Train, validation and test folders (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. There are complete map sheet images. Those images can be large (10000x10000 pixels). example: 2-segmaparea/train/101-INPUT.jpg ${SUBSET}/${NNN}-OUTPUT-GT.csv : CSV file containing a list of intersection coordinates (detailed below). example: 2-segmaparea/train/101-OUTPUT-GT.csv","title":"File naming conventions"},{"location":"tasks/task3/#number-of-elements-per-set","text":"train: 26 images validation: 6 images test: 97 images","title":"Number of elements per set"},{"location":"tasks/task3/#csv-file-format","text":"first line: header (always x,y ) other lines: two floats delimiter: comma ( , ) float format: dot ( . ) as decimal separator, 1 digit after the dot (sub-pixel accuracy) coordinate system: image: (0,0) is at top left, x axis points to the right and y axis points downward CSV example: x,y 2379.0,2338.0 2373.2,4708.2 4744.2,2332.8 4736.5,4724.5","title":"CSV file format"},{"location":"tasks/task3/#evaluation","text":"Evaluation tools and illustrative notebooks provide participants with more details than the summary below. Please subscribe to updates to be notified when they are available. For each map sheet (ie for each CSV result), we will compare the predicted coordinates with the expected ones. We will compute for each map sheet the number of correct predictions for each possible distance threshold: a predicted point will be considered as a correct detection if it is the closest predicted point of a ground truth (expected) point and the distance between the expected point and the predicted one is smaller than a given threshold we will consider all possible thresholds between 0 and 50 pixels, which roughly represents 20 meters on the maps and gives an upper limit over which the registration would be seriously disrupted. For each possible threshold we can compute the number of correct predictions, the number of incorrect ones (the complement of the prediction set) and the number of expected elements. This allows us to plot a F 0.5 F_{0.5} F 0 . 5 \u200b score vs threshold curve for a range of thresholds. The F \u03b2 F_{\\beta} F \u03b2 \u200b score with \u03b2 = 0.5 \\beta=0.5 \u03b2 = 0 . 5 weights recall lower than precision because for this task it takes several good detections to correct a wrong one in the final registration. We will take the area under this \" F 0.5 F_{0.5} F 0 . 5 \u200b score vs threshold \" curve as a performance indicator: such indicator blends two indicators: point detection and spatial accuracy. Finally, we will compute the average of the measures for all individual map images to produce a global indicator with a confidence measure. The resulting measure is a float value between 0 and 1. A higher value is better.","title":"Evaluation"}]}