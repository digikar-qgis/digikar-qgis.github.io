{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview Download dataset (train/val) Download evaluation tools Subscribe to updates Register Brought to you by the LASTIG team of the IGN (the French National Mapping Agency), the R&D Lab . of the EPITA (French engineering school in computer science), and the Center of Historical Studies of the EHESS (French graduate schools of social sciences). \ud83d\udcc5 Important dates From To / On Title Tasks 2020-11-18 Train and validation sets available all 2020-11-18 2021-04-05 Training phase all 2021-03-31 Registration deadline for competition participants all 2021-04-05 Test datasets available 1 2021-04-05 2021-04-09 Test phase 1 2021-04-09 Submission deadline for results 1 2021-04-12 Test datasets available 2&3 2021-04-12 2021-04-16 Test phase 2&3 2021-04-16 Submission deadline for results 2&3 2021-04-12 2021-04-23 Write short method description all 2021-04-23 Method descriptions due all 2021-07-01 Full data disclosure: Ground truth for test set, participant results all \ud83c\udfc6 About the competition \ud83d\uddfa\ufe0f This competition consists in solving several challenges which arise during the digitization of historical maps. We are particularly interested in a large map series consisting in many Paris atlases over half a century (1860's-1940's). For each year, a set of approximately 20 sheets forms a tiled view of the city. Such maps are highly detailed and very accurate even in modern standards. The graphical nature of the content is visible in the full map sheet illustrated below. A sample map sheet ( link to larger version ) This material provides a very valuable resource for historians and a rich body of scientific challenges for the document analysis and recognition (DAR) community: map-related challenges (overlapping of many information layers) and document-related ones (document preservation). Excerpt showing map-related challenges Excerpt showing document-related challenges We expect this competition to provide solutions for three levels of scientific problems: Task 1: Detection of building blocks Task 2: Segmentation of map content within map sheets Task 3: Localization of graticule lines intersections These tasks are necessary step during the digitization of historical maps, ie when gradually turning a raster image into a set of vector geometries projected onto a particular geographical coordinate reference system. Automatic approaches with good generalization power will provide an enormous gain for the Geographical Information Systems (GIS) communities looking for solutions when digitizing old maps, and create a great potential for many historical studies. \ud83d\ude80 Key strengths for participants All datasets and evaluation tools are released with an open license as soon as they are available. Winners of each task will be invited to co-author the report paper. We propose unsolved research problems with an important potential impact .","title":"Overview"},{"location":"#overview","text":"Download dataset (train/val) Download evaluation tools Subscribe to updates Register Brought to you by the LASTIG team of the IGN (the French National Mapping Agency), the R&D Lab . of the EPITA (French engineering school in computer science), and the Center of Historical Studies of the EHESS (French graduate schools of social sciences).","title":"Overview"},{"location":"#important-dates","text":"From To / On Title Tasks 2020-11-18 Train and validation sets available all 2020-11-18 2021-04-05 Training phase all 2021-03-31 Registration deadline for competition participants all 2021-04-05 Test datasets available 1 2021-04-05 2021-04-09 Test phase 1 2021-04-09 Submission deadline for results 1 2021-04-12 Test datasets available 2&3 2021-04-12 2021-04-16 Test phase 2&3 2021-04-16 Submission deadline for results 2&3 2021-04-12 2021-04-23 Write short method description all 2021-04-23 Method descriptions due all 2021-07-01 Full data disclosure: Ground truth for test set, participant results all","title":"\ud83d\udcc5 Important dates"},{"location":"#about-the-competition","text":"This competition consists in solving several challenges which arise during the digitization of historical maps. We are particularly interested in a large map series consisting in many Paris atlases over half a century (1860's-1940's). For each year, a set of approximately 20 sheets forms a tiled view of the city. Such maps are highly detailed and very accurate even in modern standards. The graphical nature of the content is visible in the full map sheet illustrated below. A sample map sheet ( link to larger version ) This material provides a very valuable resource for historians and a rich body of scientific challenges for the document analysis and recognition (DAR) community: map-related challenges (overlapping of many information layers) and document-related ones (document preservation). Excerpt showing map-related challenges Excerpt showing document-related challenges We expect this competition to provide solutions for three levels of scientific problems: Task 1: Detection of building blocks Task 2: Segmentation of map content within map sheets Task 3: Localization of graticule lines intersections These tasks are necessary step during the digitization of historical maps, ie when gradually turning a raster image into a set of vector geometries projected onto a particular geographical coordinate reference system. Automatic approaches with good generalization power will provide an enormous gain for the Geographical Information Systems (GIS) communities looking for solutions when digitizing old maps, and create a great potential for many historical studies.","title":"\ud83c\udfc6 About the competition \ud83d\uddfa\ufe0f"},{"location":"#key-strengths-for-participants","text":"All datasets and evaluation tools are released with an open license as soon as they are available. Winners of each task will be invited to co-author the report paper. We propose unsolved research problems with an important potential impact .","title":"\ud83d\ude80 Key strengths for participants"},{"location":"contact/","text":"Contact Single Contact Address You can contact the organizers by sending an email to: icdar21-mapseg-contact(at)googlegroups.com You can also create an issue on our tracker . Subscribe to Updates You can subscribe to the announcement list ( you can unsubscribe at any time ) by sending an email to: icdar21-mapseg-announcements+subscribe@googlegroups.com or by going to: https://groups.google.com/g/icdar21-mapseg-announcements Organizers Edwin Carlinet Associate professor of computer vision at the EPITA Computer Science school. Joseph Chazalon Associate professor of computer vision at the EPITA Computer Science school. Yizi Chen PhD candidate at IGN/ENSG/Univ. Gustave Eiffel and LRDE/EPITA. Bertrand Dum\u00e9nieu Research engineer in geographical information sciences and geospatial humanities at EHESS. Thierry G\u00e9raud Full professor of computer vision at the EPITA Computer Science school. LRDE Team leader. Cl\u00e9ment Mallet Senior researcher in geographical information sciences at IGN/ENSG/Univ. Gustave Eiffel. Julien Perret Senior researcher in geographical information sciences at IGN/ENSG/Univ. Gustave Eiffel and associated researcher at EHESS. Organizations LRDE / EPITA The EPITA is a renowned engineering school in computer science. Its R&D Lab. (LRDE) is specialized is efficient image processing, with a focus on document and medical image analysis. LASTIG / IGN IGN is the French National Mapping Agency, owning more than 3 centuries of map archives and one century of aerial images over France and foreign countries. The research from LASTIG (one of IGN research labs) covers the complete lifecycle of spatial data from the capture to the visualization, including modelling, integration and analysis of spatial data with a special interest on spatio-temporal reference data. CRH / EHESS EHESS is one of the most selective and prestigious graduate schools of social sciences in Paris, France. Its researchers are affiliated to 35 Research Centers covering an extensive range of disciplines: history, sociology, anthropology, economics, geography, archaeology, psychology, and linguistics.","title":"Contact"},{"location":"contact/#contact","text":"","title":"Contact"},{"location":"contact/#single-contact-address","text":"You can contact the organizers by sending an email to: icdar21-mapseg-contact(at)googlegroups.com You can also create an issue on our tracker .","title":"Single Contact Address"},{"location":"contact/#subscribe-to-updates","text":"You can subscribe to the announcement list ( you can unsubscribe at any time ) by sending an email to: icdar21-mapseg-announcements+subscribe@googlegroups.com or by going to: https://groups.google.com/g/icdar21-mapseg-announcements","title":"Subscribe to Updates"},{"location":"contact/#organizers","text":"Edwin Carlinet Associate professor of computer vision at the EPITA Computer Science school. Joseph Chazalon Associate professor of computer vision at the EPITA Computer Science school. Yizi Chen PhD candidate at IGN/ENSG/Univ. Gustave Eiffel and LRDE/EPITA. Bertrand Dum\u00e9nieu Research engineer in geographical information sciences and geospatial humanities at EHESS. Thierry G\u00e9raud Full professor of computer vision at the EPITA Computer Science school. LRDE Team leader. Cl\u00e9ment Mallet Senior researcher in geographical information sciences at IGN/ENSG/Univ. Gustave Eiffel. Julien Perret Senior researcher in geographical information sciences at IGN/ENSG/Univ. Gustave Eiffel and associated researcher at EHESS.","title":"Organizers"},{"location":"contact/#organizations","text":"LRDE / EPITA The EPITA is a renowned engineering school in computer science. Its R&D Lab. (LRDE) is specialized is efficient image processing, with a focus on document and medical image analysis. LASTIG / IGN IGN is the French National Mapping Agency, owning more than 3 centuries of map archives and one century of aerial images over France and foreign countries. The research from LASTIG (one of IGN research labs) covers the complete lifecycle of spatial data from the capture to the visualization, including modelling, integration and analysis of spatial data with a special interest on spatio-temporal reference data. CRH / EHESS EHESS is one of the most selective and prestigious graduate schools of social sciences in Paris, France. Its researchers are affiliated to 35 Research Centers covering an extensive range of disciplines: history, sociology, anthropology, economics, geography, archaeology, psychology, and linguistics.","title":"Organizations"},{"location":"downloads/","text":"Downloads Dataset (train and validation sets) The train and validation sets for tasks 1, 2 and 3 is available as an archive. You can currently download them here: Mirror 1 MD5 checksums: c54c35d132701bc5e6ddd5c70fc854b3 icdar21-mapseg-v1.0.0-trainval-20201114a.zip Dataset (test sets) The test sets (inputs only) will be released at the beginning of the test phase for each task: task 1: 2021-04-05 tasks 2 and 3: 2021-04-12 Please subscribe to updates to be notified when they are available. Evaluation tools Evaluation tools are available. You can download them using pip install -U icdar21-mapseg-eval They are open-source Python programs . Please check the documentation for the evaluation tools for more details about how to install them. Please check the documentation of each task for further details about how to use the evaluation tools for each task. Competition results Competition results computed by participants will be made available publicly on this website, so that anyone can recompute the results and derive the final ranking. Please subscribe to updates to be notified when they are available.","title":"Downloads"},{"location":"downloads/#downloads","text":"","title":"Downloads"},{"location":"downloads/#dataset-train-and-validation-sets","text":"The train and validation sets for tasks 1, 2 and 3 is available as an archive. You can currently download them here: Mirror 1 MD5 checksums: c54c35d132701bc5e6ddd5c70fc854b3 icdar21-mapseg-v1.0.0-trainval-20201114a.zip","title":"Dataset (train and validation sets)"},{"location":"downloads/#dataset-test-sets","text":"The test sets (inputs only) will be released at the beginning of the test phase for each task: task 1: 2021-04-05 tasks 2 and 3: 2021-04-12 Please subscribe to updates to be notified when they are available.","title":"Dataset (test sets)"},{"location":"downloads/#evaluation-tools","text":"Evaluation tools are available. You can download them using pip install -U icdar21-mapseg-eval They are open-source Python programs . Please check the documentation for the evaluation tools for more details about how to install them. Please check the documentation of each task for further details about how to use the evaluation tools for each task.","title":"Evaluation tools"},{"location":"downloads/#competition-results","text":"Competition results computed by participants will be made available publicly on this website, so that anyone can recompute the results and derive the final ranking. Please subscribe to updates to be notified when they are available.","title":"Competition results"},{"location":"registration/","text":"Registration Loading\u2026","title":"Registration"},{"location":"registration/#registration","text":"Loading\u2026","title":"Registration"},{"location":"rules/","text":"Competition Rules Dear participants, in order to create valuable results and provide a fair competition experience for all participants, please respect those rules: You can participate to 1, 2 or 3 tasks: they will be awarded separately. You must register and process the test set in time for your method to be evaluated. You must submit your method short description in time for your method to be included in the result paper. Your method must not include any manual annotation process. You must train on the training set or publicly available data exclusively. You must clearly indicate in your method presentation the training and validation sets you used. You should calibrate all your parameters using the validation set. Not doing so will be pointed out in the report. You may use the complete dataset for training. In particular, you can use the training sets for tasks 2 and 3 as unsupervised training examples for task 1. If you do so, please report it in your method's description. You must report in your method's description any redundancy in data that you make use of. We will not ask you to send code or binaries, but we encourage you to release your method under an open-source license after the competition.","title":"Competition Rules"},{"location":"rules/#competition-rules","text":"Dear participants, in order to create valuable results and provide a fair competition experience for all participants, please respect those rules: You can participate to 1, 2 or 3 tasks: they will be awarded separately. You must register and process the test set in time for your method to be evaluated. You must submit your method short description in time for your method to be included in the result paper. Your method must not include any manual annotation process. You must train on the training set or publicly available data exclusively. You must clearly indicate in your method presentation the training and validation sets you used. You should calibrate all your parameters using the validation set. Not doing so will be pointed out in the report. You may use the complete dataset for training. In particular, you can use the training sets for tasks 2 and 3 as unsupervised training examples for task 1. If you do so, please report it in your method's description. You must report in your method's description any redundancy in data that you make use of. We will not ask you to send code or binaries, but we encourage you to release your method under an open-source license after the competition.","title":"Competition Rules"},{"location":"tasks/task1/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } Task 1: Detect Building Blocks This task consists in detecting a set of closed shapes (building blocks) in the map image. Building blocks are coarser map objects which can regroup several elements. Detecting these objects is a critical step in the digitization of historical maps because it provides essential components of a city. Each building block is symbolized by a closed which can enclose other objects and lines. Building blocks are surrounded by streets, rivers fortification wall or others, and are never directly connected. Building blocks can sometimes be reduced to a single spacial building, symbolized by diagonal hatched areas. Given the image of a complete map sheet and a mask of the map area, you need to detect each building block, as illustrated below on a excerpt of the input. Excerpt of the input for task 1 Excerpt of the expected output for task 1 We identified the following challenges: Building blocks can have very variable sizes. They can be reduced to a single building (diagonal hatched area). Their contour can be damaged (ie non-closed). Several other layers of information can be overlaid on building block outlines (text, railways, underground lines, graticule lines among others). There may be decompositions inside a building block, increasing the number of edges to filter. Building blocks may be large empty areas only surrounded by a contour. The lack of texture information and the variable sizes may be challenging to multiscale texture methods like convolutional neural networks. Producing closed contours requires to embed strong guarantees in the method. Input The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images cropped to the relevant area for which the non-relevant area is replaced by black pixels, as illustrated below. Those images can be large (8000x8000 pixels). Sample input for task 1: non relevant pixels are replaced by black pixels. To help participants identify the non-relevant pixels, we also provide a mask image for which non-relevant pixels have value 0 and relevant pixels have value 255 , as illustrated below. Theses masks are the expected output for task 2 (cropped to the relevant part). Extra input mask for task 1 (exact same format as the output for task 2): indicates the masked content. Ground truth and Expected outputs Expected output for this task is a binary mask of the building blocks. It must be stored in PNG (lossless) format with an 8-bit single channel. Background must be indicated with pixel value 0, and building block areas with pixel value 255. We will threshold the image to discard any other value. The resulting image should look like the one below, which is the expected output for the sample input previously shown. Sample output for task 1 Results need to be output in a PNG file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.png . Dataset Content for task 1 is located in the folder named 1-detbblocks in the dataset archive. File naming conventions Train, validation and test folder (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. This image was cropped to map content and irrelevant content (masked out by the mask file described below) are set to black (RGB= (0,0,0) ). Those images can be large (10000x10000 pixels). example: 1-detbblocks/train/101-INPUT.jpg ${SUBSET}/${NNN}-INPUT-MASK.png : PNG GREY image containing a mask of the map area (same size as input). While the image was cropped to the meaningful area, some elements need to be discarded (map legend for instance). Map area is indicated by pixels of value 255 ; only predictions within this area is to be kept. example: 1-detbblocks/train/101-INPUT-MASK.png ${SUBSET}/${NNN}-OUTPUT-GT.png : PNG GREY image containing a mask of expected building blocks (same size as input). Building blocks are indicated by pixels of value 255 ; all other pixels (background and irrelevant) are set to 0 . example: 1-detbblocks/train/101-OUTPUT-GT.png Number of elements per set train: 1 image validation: 1 image test: 3 images Evaluation For each map sheet, we extract the connected components from the predicted label mask. Based on this set of regions, we compute the COCO PQ score associated to the instance segmentation returned by your system. Please note that as we have only 1 \u201cthing\u201d class and not \u201cstuff\u201d class, we provide indicators only for the building blocks class. These simplifications required a custom implementation which is fully compliant with the COCO PQ evaluation code. Metrics We report 3 indicators: COCO SQ (segmentation quality): mean IoU between matching shapes (matching shapes in reference and prediction have an IoU > 0.5). S Q \u2208 [ 0 , 1 ] SQ \\in [0,1] S Q \u2208 [ 0 , 1 ] , higher is better. COCO RQ (detection/recognition quality): detection F-score for shapes, a predicted shape is a true positive if it as an IoU > 0.5 with a reference shape. R Q \u2208 [ 0 , 1 ] RQ \\in [0,1] R Q \u2208 [ 0 , 1 ] , higher is better. COCO PQ (aggregated score): P Q = S Q \u2217 R Q PQ = SQ * RQ P Q = S Q \u2217 R Q . P Q \u2208 [ 0 , 1 ] PQ \\in [0,1] P Q \u2208 [ 0 , 1 ] , higher is better. The indicators are computed as: PQ = \u2211 ( p , g ) \u2208 T P IoU ( p , g ) 1 2 \u2223 T P \u2223 \u23df segmentation quality (SQ) \u00d7 \u2223 T P \u2223 \u2223 T P \u2223 + 1 2 \u2223 F P \u2223 + 1 2 \u2223 F N \u2223 \u23df recognition quality (RQ) {\\text{PQ}} = \\underbrace{\\frac{\\sum_{(p, g) \\in TP} \\text{IoU}(p, g)}{\\vphantom{\\frac{1}{2}}|TP|}}_{\\text{segmentation quality (SQ)}} \\times \\underbrace{\\frac{|TP|}{|TP| + \\frac{1}{2} |FP| + \\frac{1}{2} |FN|}}_{\\text{recognition quality (RQ)}} PQ = segmentation quality (SQ) 2 1 \u200b \u2223 T P \u2223 \u2211 ( p , g ) \u2208 T P \u200b IoU ( p , g ) \u200b \u200b \u200b \u00d7 recognition quality (RQ) \u2223 T P \u2223 + 2 1 \u200b \u2223 F P \u2223 + 2 1 \u200b \u2223 F N \u2223 \u2223 T P \u2223 \u200b \u200b \u200b where T P TP T P is the set of matching pairs ( p , g ) \u2208 ( P \u00d7 G ) (p, g) \\in (P \\times G) ( p , g ) \u2208 ( P \u00d7 G ) between predictions ( P P P ) and reference ( G G G ), F P FP F P is the set of unmatched predicted shapes, and F N FN F N is the set of unmatched reference shapes. Shapes are considered as matching when: IoU ( p , g ) = p \u2229 g p \u222a g > 0.5. \\text{IoU}(p,g) = \\frac{p \\cap g}{p \\cup g} \\gt 0.5. IoU ( p , g ) = p \u222a g p \u2229 g \u200b > 0 . 5 . Using the evaluation tool The evaluation tool supports comparing either: a predicted segmentation to a reference segmentation (as two binary images in PNG or two label maps in TIFF16). a reference directory to a reference segmentation. In this case, reference files are expected to end with -OUTPUT-GT.png , and prediction files with -OUTPUT-PRED.png or -OUTPUT-*.tiff . Comparing two files: $ icdar21-mapseg-eval T1 201-OUTPUT-GT.png 201-OUTPUT-PRED.png output_dir 201-OUTPUT-PRED.png - COCO PQ 1.00 = 1.00 SQ * 1.00 RQ Comparing two directories: $ icdar21-mapseg-eval T1 1-detbblocks/validation/ mypred/t1/validation/ output_dir Processing |################################| 1/1 COCO PQ COCO SQ COCO RQ Reference Prediction 201-OUTPUT-GT.png 201-OUTPUT-PRED.png 1.0 1.0 1.0 ============================== Global score for task 1: 1.000 ============================ Files generated in output folder The output directory will contain something like: 201-OUTPUT-GT.plot.pdf global_coco.csv global_score.json Detail: global_coco.csv : COCO metrics for each image. global_score.json : Easy to parse file for global score with a summary of files analyzed. NNN-OUTPUT-PRED.plot.pdf : Plot of the F-score against all IoU thresholds (COCO PQ is the area under the F-score curve + the value of the F-score at 0.5).","title":"Task 1: Detect Building Blocks"},{"location":"tasks/task1/#task-1-detect-building-blocks","text":"This task consists in detecting a set of closed shapes (building blocks) in the map image. Building blocks are coarser map objects which can regroup several elements. Detecting these objects is a critical step in the digitization of historical maps because it provides essential components of a city. Each building block is symbolized by a closed which can enclose other objects and lines. Building blocks are surrounded by streets, rivers fortification wall or others, and are never directly connected. Building blocks can sometimes be reduced to a single spacial building, symbolized by diagonal hatched areas. Given the image of a complete map sheet and a mask of the map area, you need to detect each building block, as illustrated below on a excerpt of the input. Excerpt of the input for task 1 Excerpt of the expected output for task 1 We identified the following challenges: Building blocks can have very variable sizes. They can be reduced to a single building (diagonal hatched area). Their contour can be damaged (ie non-closed). Several other layers of information can be overlaid on building block outlines (text, railways, underground lines, graticule lines among others). There may be decompositions inside a building block, increasing the number of edges to filter. Building blocks may be large empty areas only surrounded by a contour. The lack of texture information and the variable sizes may be challenging to multiscale texture methods like convolutional neural networks. Producing closed contours requires to embed strong guarantees in the method.","title":"Task 1: Detect Building Blocks"},{"location":"tasks/task1/#input","text":"The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images cropped to the relevant area for which the non-relevant area is replaced by black pixels, as illustrated below. Those images can be large (8000x8000 pixels). Sample input for task 1: non relevant pixels are replaced by black pixels. To help participants identify the non-relevant pixels, we also provide a mask image for which non-relevant pixels have value 0 and relevant pixels have value 255 , as illustrated below. Theses masks are the expected output for task 2 (cropped to the relevant part). Extra input mask for task 1 (exact same format as the output for task 2): indicates the masked content.","title":"Input"},{"location":"tasks/task1/#ground-truth-and-expected-outputs","text":"Expected output for this task is a binary mask of the building blocks. It must be stored in PNG (lossless) format with an 8-bit single channel. Background must be indicated with pixel value 0, and building block areas with pixel value 255. We will threshold the image to discard any other value. The resulting image should look like the one below, which is the expected output for the sample input previously shown. Sample output for task 1 Results need to be output in a PNG file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.png .","title":"Ground truth and Expected outputs"},{"location":"tasks/task1/#dataset","text":"Content for task 1 is located in the folder named 1-detbblocks in the dataset archive.","title":"Dataset"},{"location":"tasks/task1/#file-naming-conventions","text":"Train, validation and test folder (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. This image was cropped to map content and irrelevant content (masked out by the mask file described below) are set to black (RGB= (0,0,0) ). Those images can be large (10000x10000 pixels). example: 1-detbblocks/train/101-INPUT.jpg ${SUBSET}/${NNN}-INPUT-MASK.png : PNG GREY image containing a mask of the map area (same size as input). While the image was cropped to the meaningful area, some elements need to be discarded (map legend for instance). Map area is indicated by pixels of value 255 ; only predictions within this area is to be kept. example: 1-detbblocks/train/101-INPUT-MASK.png ${SUBSET}/${NNN}-OUTPUT-GT.png : PNG GREY image containing a mask of expected building blocks (same size as input). Building blocks are indicated by pixels of value 255 ; all other pixels (background and irrelevant) are set to 0 . example: 1-detbblocks/train/101-OUTPUT-GT.png","title":"File naming conventions"},{"location":"tasks/task1/#number-of-elements-per-set","text":"train: 1 image validation: 1 image test: 3 images","title":"Number of elements per set"},{"location":"tasks/task1/#evaluation","text":"For each map sheet, we extract the connected components from the predicted label mask. Based on this set of regions, we compute the COCO PQ score associated to the instance segmentation returned by your system. Please note that as we have only 1 \u201cthing\u201d class and not \u201cstuff\u201d class, we provide indicators only for the building blocks class. These simplifications required a custom implementation which is fully compliant with the COCO PQ evaluation code.","title":"Evaluation"},{"location":"tasks/task1/#metrics","text":"We report 3 indicators: COCO SQ (segmentation quality): mean IoU between matching shapes (matching shapes in reference and prediction have an IoU > 0.5). S Q \u2208 [ 0 , 1 ] SQ \\in [0,1] S Q \u2208 [ 0 , 1 ] , higher is better. COCO RQ (detection/recognition quality): detection F-score for shapes, a predicted shape is a true positive if it as an IoU > 0.5 with a reference shape. R Q \u2208 [ 0 , 1 ] RQ \\in [0,1] R Q \u2208 [ 0 , 1 ] , higher is better. COCO PQ (aggregated score): P Q = S Q \u2217 R Q PQ = SQ * RQ P Q = S Q \u2217 R Q . P Q \u2208 [ 0 , 1 ] PQ \\in [0,1] P Q \u2208 [ 0 , 1 ] , higher is better. The indicators are computed as: PQ = \u2211 ( p , g ) \u2208 T P IoU ( p , g ) 1 2 \u2223 T P \u2223 \u23df segmentation quality (SQ) \u00d7 \u2223 T P \u2223 \u2223 T P \u2223 + 1 2 \u2223 F P \u2223 + 1 2 \u2223 F N \u2223 \u23df recognition quality (RQ) {\\text{PQ}} = \\underbrace{\\frac{\\sum_{(p, g) \\in TP} \\text{IoU}(p, g)}{\\vphantom{\\frac{1}{2}}|TP|}}_{\\text{segmentation quality (SQ)}} \\times \\underbrace{\\frac{|TP|}{|TP| + \\frac{1}{2} |FP| + \\frac{1}{2} |FN|}}_{\\text{recognition quality (RQ)}} PQ = segmentation quality (SQ) 2 1 \u200b \u2223 T P \u2223 \u2211 ( p , g ) \u2208 T P \u200b IoU ( p , g ) \u200b \u200b \u200b \u00d7 recognition quality (RQ) \u2223 T P \u2223 + 2 1 \u200b \u2223 F P \u2223 + 2 1 \u200b \u2223 F N \u2223 \u2223 T P \u2223 \u200b \u200b \u200b where T P TP T P is the set of matching pairs ( p , g ) \u2208 ( P \u00d7 G ) (p, g) \\in (P \\times G) ( p , g ) \u2208 ( P \u00d7 G ) between predictions ( P P P ) and reference ( G G G ), F P FP F P is the set of unmatched predicted shapes, and F N FN F N is the set of unmatched reference shapes. Shapes are considered as matching when: IoU ( p , g ) = p \u2229 g p \u222a g > 0.5. \\text{IoU}(p,g) = \\frac{p \\cap g}{p \\cup g} \\gt 0.5. IoU ( p , g ) = p \u222a g p \u2229 g \u200b > 0 . 5 .","title":"Metrics"},{"location":"tasks/task1/#using-the-evaluation-tool","text":"The evaluation tool supports comparing either: a predicted segmentation to a reference segmentation (as two binary images in PNG or two label maps in TIFF16). a reference directory to a reference segmentation. In this case, reference files are expected to end with -OUTPUT-GT.png , and prediction files with -OUTPUT-PRED.png or -OUTPUT-*.tiff . Comparing two files: $ icdar21-mapseg-eval T1 201-OUTPUT-GT.png 201-OUTPUT-PRED.png output_dir 201-OUTPUT-PRED.png - COCO PQ 1.00 = 1.00 SQ * 1.00 RQ Comparing two directories: $ icdar21-mapseg-eval T1 1-detbblocks/validation/ mypred/t1/validation/ output_dir Processing |################################| 1/1 COCO PQ COCO SQ COCO RQ Reference Prediction 201-OUTPUT-GT.png 201-OUTPUT-PRED.png 1.0 1.0 1.0 ============================== Global score for task 1: 1.000 ============================","title":"Using the evaluation tool"},{"location":"tasks/task1/#files-generated-in-output-folder","text":"The output directory will contain something like: 201-OUTPUT-GT.plot.pdf global_coco.csv global_score.json Detail: global_coco.csv : COCO metrics for each image. global_score.json : Easy to parse file for global score with a summary of files analyzed. NNN-OUTPUT-PRED.plot.pdf : Plot of the F-score against all IoU thresholds (COCO PQ is the area under the F-score curve + the value of the F-score at 0.5).","title":"Files generated in output folder"},{"location":"tasks/task2/","text":"Task 2: Segment Map Content Area This task consists in segmenting the map content from the rest of the sheet. This is a rather classical document analysis task as it consist in focusing on the relevant area in order to perform a dedicated analysis. In our case, task 1 would be the following stage in the pipeline. Given the image of a complete map sheet, you need to locate the boundary of the map content, as illustrated below. Illustration of expected outputs for task 2: green area is the map content and red hatched area is the background. We identified the following challenges: The map area is usually well separated from the other elements (title, legend, scale\u2026) by several frames but sometimes map contents exceeds the frame for some large objects. The frame itself is not straight and can be damaged. Input Note that the inputs for this task are the same as task 3. The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images. Those images can be large (10000x10000 pixels). Sample input for task 3 Ground truth and Expected outputs Expected output for this task is a binary mask of the map content area. It must be stored in PNG (lossless) format with an 8-bit single channel. Background must be indicated with pixel value 0, and map content area with pixel value 255. We will threshold the image to discard any other value. The resulting image should look like the one below, which is the expected output for the sample input previously shown. Sample output for task 2 Results need to be output in a PNG file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.png . Dataset Content for task 2 is located in the folder named 2-segmaparea in the dataset archive. File naming conventions Train, validation and test folder (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. There are complete map sheet images. Those images can be large (10000x10000 pixels). example: 2-segmaparea/train/101-INPUT.jpg ${SUBSET}/${NNN}-OUTPUT-GT.png : PNG GREY image containing a mask of expected map are (same size as input). Map area is indicated by pixels of value 255 ; all other pixels are set to 0 . example: 2-segmaparea/train/101-OUTPUT-GT.png Number of elements per set train: 26 images validation: 6 images test: 97 images Evaluation To evaluate the quality of the segmentation, we will compute the Hausdorff distance between the expected shape do detect and the predicted one. This implies that there is only one (connected, without hole) shape to detect. This measure has the advantage over the \u201cintersection over union\u201d, \u201cJaccard index\u201d and other area measures that it keeps a good \u201ccontrast\u201d between results in the case of large objects (because there is no normalization by the area). Metric More specifically, we will use the \u201cHausdorff 95\u201d variant which discards the 5 percentiles of higher values (assumed to be outliers) to produce a more stable measure. Finally, we will compute the average of the measures for all individual map images to produce a global indicator with a confidence measure. The resulting measure is a float value between 0 and a large value. A lower value is better. Tool sample usage The evaluation tool supports comparing either: a predicted segmentation to a reference segmentation (as two binary images) a reference directory to a reference segmentation In this case, reference files are expected to end with -OUTPUT-GT.png , and prediction files with -OUTPUT-PRED.png . Comparing two files: $ icdar21-mapseg-eval T2 201-OUTPUT-GT.png 201-OUTPUT-PRED.png output_dir 201-OUTPUT-PRED.png - Haussdorff95 = 0.00 Comparing two directories: $ icdar21-mapseg-eval T2 ./2-segmaparea/validation mypred/t2/validation output_dir .../PIL/Image.py:2847: DecompressionBombWarning: Image size (137239200 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack. Processing |################################| 6/6 Error Reference Prediction 201-OUTPUT-GT.png 201-OUTPUT-GT.png 0.0 202-OUTPUT-GT.png 202-OUTPUT-GT.png 0.0 203-OUTPUT-GT.png 203-OUTPUT-GT.png 0.0 204-OUTPUT-GT.png 204-OUTPUT-GT.png 0.0 205-OUTPUT-GT.png 205-OUTPUT-GT.png 0.0 206-OUTPUT-GT.png 206-OUTPUT-GT.png 0.0 ============================== Global error for task 2: 0.000 Because the PNG files are large, you may get a warning from PIP that you can safely ignore: DecompressionBombWarning: Image size (137239200 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack. Files generated in output folder When processing directories, the output directory will contain the following files: global_error.json : Easy to parse file for global score with a summary of files analyzed. global_hd95.csv : HD95 metrics for each image.","title":"Task 2: Segment Map Content Area"},{"location":"tasks/task2/#task-2-segment-map-content-area","text":"This task consists in segmenting the map content from the rest of the sheet. This is a rather classical document analysis task as it consist in focusing on the relevant area in order to perform a dedicated analysis. In our case, task 1 would be the following stage in the pipeline. Given the image of a complete map sheet, you need to locate the boundary of the map content, as illustrated below. Illustration of expected outputs for task 2: green area is the map content and red hatched area is the background. We identified the following challenges: The map area is usually well separated from the other elements (title, legend, scale\u2026) by several frames but sometimes map contents exceeds the frame for some large objects. The frame itself is not straight and can be damaged.","title":"Task 2: Segment Map Content Area"},{"location":"tasks/task2/#input","text":"Note that the inputs for this task are the same as task 3. The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images. Those images can be large (10000x10000 pixels). Sample input for task 3","title":"Input"},{"location":"tasks/task2/#ground-truth-and-expected-outputs","text":"Expected output for this task is a binary mask of the map content area. It must be stored in PNG (lossless) format with an 8-bit single channel. Background must be indicated with pixel value 0, and map content area with pixel value 255. We will threshold the image to discard any other value. The resulting image should look like the one below, which is the expected output for the sample input previously shown. Sample output for task 2 Results need to be output in a PNG file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.png .","title":"Ground truth and Expected outputs"},{"location":"tasks/task2/#dataset","text":"Content for task 2 is located in the folder named 2-segmaparea in the dataset archive.","title":"Dataset"},{"location":"tasks/task2/#file-naming-conventions","text":"Train, validation and test folder (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. There are complete map sheet images. Those images can be large (10000x10000 pixels). example: 2-segmaparea/train/101-INPUT.jpg ${SUBSET}/${NNN}-OUTPUT-GT.png : PNG GREY image containing a mask of expected map are (same size as input). Map area is indicated by pixels of value 255 ; all other pixels are set to 0 . example: 2-segmaparea/train/101-OUTPUT-GT.png","title":"File naming conventions"},{"location":"tasks/task2/#number-of-elements-per-set","text":"train: 26 images validation: 6 images test: 97 images","title":"Number of elements per set"},{"location":"tasks/task2/#evaluation","text":"To evaluate the quality of the segmentation, we will compute the Hausdorff distance between the expected shape do detect and the predicted one. This implies that there is only one (connected, without hole) shape to detect. This measure has the advantage over the \u201cintersection over union\u201d, \u201cJaccard index\u201d and other area measures that it keeps a good \u201ccontrast\u201d between results in the case of large objects (because there is no normalization by the area).","title":"Evaluation"},{"location":"tasks/task2/#metric","text":"More specifically, we will use the \u201cHausdorff 95\u201d variant which discards the 5 percentiles of higher values (assumed to be outliers) to produce a more stable measure. Finally, we will compute the average of the measures for all individual map images to produce a global indicator with a confidence measure. The resulting measure is a float value between 0 and a large value. A lower value is better.","title":"Metric"},{"location":"tasks/task2/#tool-sample-usage","text":"The evaluation tool supports comparing either: a predicted segmentation to a reference segmentation (as two binary images) a reference directory to a reference segmentation In this case, reference files are expected to end with -OUTPUT-GT.png , and prediction files with -OUTPUT-PRED.png . Comparing two files: $ icdar21-mapseg-eval T2 201-OUTPUT-GT.png 201-OUTPUT-PRED.png output_dir 201-OUTPUT-PRED.png - Haussdorff95 = 0.00 Comparing two directories: $ icdar21-mapseg-eval T2 ./2-segmaparea/validation mypred/t2/validation output_dir .../PIL/Image.py:2847: DecompressionBombWarning: Image size (137239200 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack. Processing |################################| 6/6 Error Reference Prediction 201-OUTPUT-GT.png 201-OUTPUT-GT.png 0.0 202-OUTPUT-GT.png 202-OUTPUT-GT.png 0.0 203-OUTPUT-GT.png 203-OUTPUT-GT.png 0.0 204-OUTPUT-GT.png 204-OUTPUT-GT.png 0.0 205-OUTPUT-GT.png 205-OUTPUT-GT.png 0.0 206-OUTPUT-GT.png 206-OUTPUT-GT.png 0.0 ============================== Global error for task 2: 0.000 Because the PNG files are large, you may get a warning from PIP that you can safely ignore: DecompressionBombWarning: Image size (137239200 pixels) exceeds limit of 89478485 pixels, could be decompression bomb DOS attack.","title":"Tool sample usage"},{"location":"tasks/task2/#files-generated-in-output-folder","text":"When processing directories, the output directory will contain the following files: global_error.json : Easy to parse file for global score with a summary of files analyzed. global_hd95.csv : HD95 metrics for each image.","title":"Files generated in output folder"},{"location":"tasks/task3/","text":".katex img { display: block; position: absolute; width: 100%; height: inherit; } Task 3: Locate Graticule Lines Intersections This task consists in locating the intersection points of graticule lines. Graticule lines are lines indicating the North/South/East/West major coordinates in the map. They are drawn every 1000 meters in each direction and overlap with the rest of the map content. Their intersections are very useful to geo-reference the map image, ie for projecting map content in a modern geographical coordinate reference system . Given the image of a complete map sheet, you need to locate the intersection points of such lines, as illustrated below. Illustration of expected outputs for task 3: dashed green lines are the graticule lines and red dots are the intersections points to locate. We identified the following challenges: Theses lines, while crossing the map from edge to edge, may not be horizontal and vertical but sometimes diagonal. They can be damaged and locally curbed. They overlap numerous map content and can touch other parallel segments. Input Note that the inputs for this task are the same as task 2. The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images. Those images can be large (10000x10000 pixels). Sample input for task 3 Close view of a sample input for task 3 Ground truth and Expected outputs Expected output for this task is a list of coordinates (in image referential, ie. 0,0 at top left, x axis pointing to the right and y axis pointing downward). Coordinates need to be output in a CSV file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.csv . The format of the CSV is the same as the one of the ground truth described below. The CSV should look like: x,y 2379.0,2338.0 2373.2,4708.2 4744.2,2332.8 4736.5,4724.5 Each CSV line should indicate an intersection like the one illustrated below. Close view of an intersection point to detection for task 3: you can see that many graphical elements can disrupt the detection of such element. Dataset Content for task 3 is located in the folder named 3-locglinesinter in the dataset archive. WARNING: because the inputs of this task are exactly the same as task 2, we did not duplicate them. Please copy or link the 2-segmaparea/{train,validation,test}/*-INPUT.jpg files accordingly. File naming conventions Train, validation and test folders (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. There are complete map sheet images. Those images can be large (10000x10000 pixels). example: 2-segmaparea/train/101-INPUT.jpg ${SUBSET}/${NNN}-OUTPUT-GT.csv : CSV file containing a list of intersection coordinates (detailed below). example: 2-segmaparea/train/101-OUTPUT-GT.csv Number of elements per set train: 26 images validation: 6 images test: 97 images CSV file format first line: header (always x,y ) other lines: two floats delimiter: comma ( , ) float format: dot ( . ) as decimal separator, 1 digit after the dot (sub-pixel accuracy) coordinate system: image: (0,0) is at top left, x axis points to the right and y axis points downward CSV example: x,y 2379.0,2338.0 2373.2,4708.2 4744.2,2332.8 4736.5,4724.5 Evaluation For each map sheet (i.e. for each CSV result), we will compare the predicted coordinates with the expected ones. We will report an aggregated indicator ( PDS for \u201cPoints Detection Score\u201d) which considers detection and localization accuracy simultaneously. Metric We will compute for each map sheet the number of correct predictions for each possible distance threshold: a predicted point will be considered as a correct detection if it is the closest predicted point of a ground truth (expected) point and the distance between the expected point and the predicted one is smaller than a given threshold we will consider all possible thresholds between 0 and 50 pixels, which roughly represents 20 meters on the maps and gives an upper limit over which the registration would be seriously disrupted. For each possible threshold we can compute the number of correct predictions, the number of incorrect ones (the complement of the prediction set) and the number of expected elements. This allows us to plot a F \u03b2 F_{\\beta} F \u03b2 \u200b score vs threshold curve for a range of thresholds. The F \u03b2 F_{\\beta} F \u03b2 \u200b score with \u03b2 = 0.5 \\beta=0.5 \u03b2 = 0 . 5 weights recall lower than precision because for this task it takes several good detections to correct a wrong one in the final registration. We will take the area under this \u201c F 0.5 F_{0.5} F 0 . 5 \u200b score vs threshold \u201d curve as a performance indicator: such indicator blends two indicators: point detection and spatial accuracy. Finally, we will compute the average of the measures for all individual map images to produce a global indicator with a confidence measure. The resulting measure is a float value between 0 and 1. A higher value is better. Tool sample usage The evaluation tool supports comparing either: a predicted detection to a reference detection (as two CSV files) a reference directory to a reference detection In this case, reference files are expected to end with -OUTPUT-GT.csv , and prediction files with -OUTPUT-PRED.csv . Comparing two files: $ icdar21-mapseg-eval T3 201-OUTPUT-GT.csv 201-OUTPUT-PRED.csv output_dir 201-OUTPUT-PRED.csv - Score: 1.000 Comparing two directories: $ icdar21-mapseg-eval T3 ./3-locglinesinter/validation mypred/t3/validation output_dir Processing |################################| 6/6 Score Reference Predictions 201-OUTPUT-GT.csv 201-OUTPUT-PRED.csv 1.0 202-OUTPUT-GT.csv 202-OUTPUT-PRED.csv 1.0 203-OUTPUT-GT.csv 203-OUTPUT-PRED.csv 1.0 204-OUTPUT-GT.csv 204-OUTPUT-PRED.csv 1.0 205-OUTPUT-GT.csv 205-OUTPUT-PRED.csv 1.0 206-OUTPUT-GT.csv 206-OUTPUT-PRED.csv 1.0 ============================== Global score for task 3: 1.000 ============================== Files generated in output folder The output directory will contain something like: 201-OUTPUT-PRED.clf.pdf 201-OUTPUT-PRED.eval.csv 201-OUTPUT-PRED.plot.csv 201-OUTPUT-PRED.plot.pdf ... global_rad:50_beta:0.50.csv global_score.json Detail: global_rad:50_beta:0.50.csv : global score for each pair of files (ground truth, prediction). global_score.json : Easy to parse file for global score with a summary of files analyzed, and values for evaluation parameters. nnn-OUTPUT-PRED.eval.csv : CSV file with all intermediate metrics (precision, recall, f_beta, tps, fns, fps, etc.) computed for each detected point. nnn-OUTPUT-PRED.plot.csv : Source values used to generate the curve to plot. nnn-OUTPUT-PRED.plot.pdf : Plot of the curve used to compute the global metric. nnn-OUTPUT-PRED.clf.pdf : A visualization of predictions and their error classification against the ground truth. You can check the Demo analysis notebook for task 3 for further details about the evaluation tools for task 3.","title":"Task 3: Locate Graticule Lines Intersections"},{"location":"tasks/task3/#task-3-locate-graticule-lines-intersections","text":"This task consists in locating the intersection points of graticule lines. Graticule lines are lines indicating the North/South/East/West major coordinates in the map. They are drawn every 1000 meters in each direction and overlap with the rest of the map content. Their intersections are very useful to geo-reference the map image, ie for projecting map content in a modern geographical coordinate reference system . Given the image of a complete map sheet, you need to locate the intersection points of such lines, as illustrated below. Illustration of expected outputs for task 3: dashed green lines are the graticule lines and red dots are the intersections points to locate. We identified the following challenges: Theses lines, while crossing the map from edge to edge, may not be horizontal and vertical but sometimes diagonal. They can be damaged and locally curbed. They overlap numerous map content and can touch other parallel segments.","title":"Task 3: Locate Graticule Lines Intersections"},{"location":"tasks/task3/#input","text":"Note that the inputs for this task are the same as task 2. The inputs form a set of JPEG RGB images like the one illustrated below. There are complete map sheet images. Those images can be large (10000x10000 pixels). Sample input for task 3 Close view of a sample input for task 3","title":"Input"},{"location":"tasks/task3/#ground-truth-and-expected-outputs","text":"Expected output for this task is a list of coordinates (in image referential, ie. 0,0 at top left, x axis pointing to the right and y axis pointing downward). Coordinates need to be output in a CSV file with the exact same format and naming conventions as the ground truth, except for the GT part of the filename which should be changed into PRED : if the input image is named train/301-INPUT.jpg , then the output file must be named train/301-OUTPUT-PRED.csv . The format of the CSV is the same as the one of the ground truth described below. The CSV should look like: x,y 2379.0,2338.0 2373.2,4708.2 4744.2,2332.8 4736.5,4724.5 Each CSV line should indicate an intersection like the one illustrated below. Close view of an intersection point to detection for task 3: you can see that many graphical elements can disrupt the detection of such element.","title":"Ground truth and Expected outputs"},{"location":"tasks/task3/#dataset","text":"Content for task 3 is located in the folder named 3-locglinesinter in the dataset archive. WARNING: because the inputs of this task are exactly the same as task 2, we did not duplicate them. Please copy or link the 2-segmaparea/{train,validation,test}/*-INPUT.jpg files accordingly.","title":"Dataset"},{"location":"tasks/task3/#file-naming-conventions","text":"Train, validation and test folders (if applicable) contain the same kind of files: ${SUBSET}/${NNN}-INPUT.jpg : JPEG RGB image containing the input image to process. There are complete map sheet images. Those images can be large (10000x10000 pixels). example: 2-segmaparea/train/101-INPUT.jpg ${SUBSET}/${NNN}-OUTPUT-GT.csv : CSV file containing a list of intersection coordinates (detailed below). example: 2-segmaparea/train/101-OUTPUT-GT.csv","title":"File naming conventions"},{"location":"tasks/task3/#number-of-elements-per-set","text":"train: 26 images validation: 6 images test: 97 images","title":"Number of elements per set"},{"location":"tasks/task3/#csv-file-format","text":"first line: header (always x,y ) other lines: two floats delimiter: comma ( , ) float format: dot ( . ) as decimal separator, 1 digit after the dot (sub-pixel accuracy) coordinate system: image: (0,0) is at top left, x axis points to the right and y axis points downward CSV example: x,y 2379.0,2338.0 2373.2,4708.2 4744.2,2332.8 4736.5,4724.5","title":"CSV file format"},{"location":"tasks/task3/#evaluation","text":"For each map sheet (i.e. for each CSV result), we will compare the predicted coordinates with the expected ones. We will report an aggregated indicator ( PDS for \u201cPoints Detection Score\u201d) which considers detection and localization accuracy simultaneously.","title":"Evaluation"},{"location":"tasks/task3/#metric","text":"We will compute for each map sheet the number of correct predictions for each possible distance threshold: a predicted point will be considered as a correct detection if it is the closest predicted point of a ground truth (expected) point and the distance between the expected point and the predicted one is smaller than a given threshold we will consider all possible thresholds between 0 and 50 pixels, which roughly represents 20 meters on the maps and gives an upper limit over which the registration would be seriously disrupted. For each possible threshold we can compute the number of correct predictions, the number of incorrect ones (the complement of the prediction set) and the number of expected elements. This allows us to plot a F \u03b2 F_{\\beta} F \u03b2 \u200b score vs threshold curve for a range of thresholds. The F \u03b2 F_{\\beta} F \u03b2 \u200b score with \u03b2 = 0.5 \\beta=0.5 \u03b2 = 0 . 5 weights recall lower than precision because for this task it takes several good detections to correct a wrong one in the final registration. We will take the area under this \u201c F 0.5 F_{0.5} F 0 . 5 \u200b score vs threshold \u201d curve as a performance indicator: such indicator blends two indicators: point detection and spatial accuracy. Finally, we will compute the average of the measures for all individual map images to produce a global indicator with a confidence measure. The resulting measure is a float value between 0 and 1. A higher value is better.","title":"Metric"},{"location":"tasks/task3/#tool-sample-usage","text":"The evaluation tool supports comparing either: a predicted detection to a reference detection (as two CSV files) a reference directory to a reference detection In this case, reference files are expected to end with -OUTPUT-GT.csv , and prediction files with -OUTPUT-PRED.csv . Comparing two files: $ icdar21-mapseg-eval T3 201-OUTPUT-GT.csv 201-OUTPUT-PRED.csv output_dir 201-OUTPUT-PRED.csv - Score: 1.000 Comparing two directories: $ icdar21-mapseg-eval T3 ./3-locglinesinter/validation mypred/t3/validation output_dir Processing |################################| 6/6 Score Reference Predictions 201-OUTPUT-GT.csv 201-OUTPUT-PRED.csv 1.0 202-OUTPUT-GT.csv 202-OUTPUT-PRED.csv 1.0 203-OUTPUT-GT.csv 203-OUTPUT-PRED.csv 1.0 204-OUTPUT-GT.csv 204-OUTPUT-PRED.csv 1.0 205-OUTPUT-GT.csv 205-OUTPUT-PRED.csv 1.0 206-OUTPUT-GT.csv 206-OUTPUT-PRED.csv 1.0 ============================== Global score for task 3: 1.000 ==============================","title":"Tool sample usage"},{"location":"tasks/task3/#files-generated-in-output-folder","text":"The output directory will contain something like: 201-OUTPUT-PRED.clf.pdf 201-OUTPUT-PRED.eval.csv 201-OUTPUT-PRED.plot.csv 201-OUTPUT-PRED.plot.pdf ... global_rad:50_beta:0.50.csv global_score.json Detail: global_rad:50_beta:0.50.csv : global score for each pair of files (ground truth, prediction). global_score.json : Easy to parse file for global score with a summary of files analyzed, and values for evaluation parameters. nnn-OUTPUT-PRED.eval.csv : CSV file with all intermediate metrics (precision, recall, f_beta, tps, fns, fps, etc.) computed for each detected point. nnn-OUTPUT-PRED.plot.csv : Source values used to generate the curve to plot. nnn-OUTPUT-PRED.plot.pdf : Plot of the curve used to compute the global metric. nnn-OUTPUT-PRED.clf.pdf : A visualization of predictions and their error classification against the ground truth. You can check the Demo analysis notebook for task 3 for further details about the evaluation tools for task 3.","title":"Files generated in output folder"}]}